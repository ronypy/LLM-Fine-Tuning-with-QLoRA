{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ LLM Fine-Tuning with QLoRA â€” Interactive Training Notebook\n",
    "\n",
    "This notebook walks you through **every step** of fine-tuning a Large Language Model (Mistral-7B) on a **text-to-SQL** task using **QLoRA** (Quantised Low-Rank Adaptation).\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "| Step | Concept | Why It Matters |\n",
    "|------|---------|----------------|\n",
    "| 1 | Environment Setup | Install the right versions of everything |\n",
    "| 2 | Dataset Loading | Use HF `datasets` to load text-to-SQL data |\n",
    "| 3 | Prompt Engineering | Format data as instruction-style prompts |\n",
    "| 4 | Quantisation (BitsAndBytes) | Compress 7B model from 14 GB â†’ 4.5 GB |\n",
    "| 5 | LoRA Adapter Setup | Train only 0.4% of parameters |\n",
    "| 6 | Training with SFTTrainer | Supervised fine-tuning loop |\n",
    "| 7 | Inference | Generate SQL from natural language |\n",
    "| 8 | Evaluation | Measure perplexity, exact match, BLEU |\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:**\n",
    "- A CUDA-capable GPU (16+ GB VRAM recommended, e.g. T4, A10, L4, A100)\n",
    "- Python 3.9+\n",
    "- Hugging Face account (for model access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Install all dependencies. If running on Google Colab, the cell below handles everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Install Dependencies\n",
    "# ============================================================================\n",
    "# We install pinned versions to ensure reproducibility.\n",
    "# On Colab, many of these are pre-installed but may need upgrading.\n",
    "\n",
    "!pip install -q \\\n",
    "    transformers>=4.36.0 \\\n",
    "    datasets>=2.16.0 \\\n",
    "    accelerate>=0.25.0 \\\n",
    "    peft>=0.7.0 \\\n",
    "    trl>=0.7.0 \\\n",
    "    bitsandbytes>=0.41.0 \\\n",
    "    wandb>=0.16.0 \\\n",
    "    evaluate>=0.4.0 \\\n",
    "    rouge-score \\\n",
    "    sqlparse \\\n",
    "    rich \\\n",
    "    pyyaml\n",
    "\n",
    "# Verify GPU is available\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available:  {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:             {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM:            {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1b: (Optional) Login to Hugging Face & Weights and Biases\n",
    "# ============================================================================\n",
    "# Hugging Face login is needed to download gated models (e.g. Llama-2).\n",
    "# Mistral-7B is NOT gated, so you can skip the HF login.\n",
    "\n",
    "# Uncomment if you need HF login:\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "\n",
    "# Uncomment for W&B tracking:\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Dataset\n",
    "\n",
    "We use the **b-mc2/sql-create-context** dataset â€” a curated collection of natural-language questions paired with SQL schemas and gold SQL queries.\n",
    "\n",
    "Each example has:\n",
    "- `question`: \"How many employees are in department 5?\"\n",
    "- `context`: `CREATE TABLE employees (id INT, name TEXT, dept_id INT)`\n",
    "- `answer`: `SELECT COUNT(*) FROM employees WHERE dept_id = 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Load Dataset from Hugging Face Hub\n",
    "# ============================================================================\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load_dataset() downloads from https://huggingface.co/datasets/b-mc2/sql-create-context\n",
    "# It caches the data locally (~50 MB) so re-runs are instant.\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "\n",
    "print(f\"Total examples: {len(dataset):,}\")\n",
    "print(f\"Columns:        {dataset.column_names}\")\n",
    "print(f\"\\nSample example:\")\n",
    "print(f\"  Question: {dataset[0]['question']}\")\n",
    "print(f\"  Schema:   {dataset[0]['context'][:100]}...\")\n",
    "print(f\"  Answer:   {dataset[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2b: Subsample for Quick Experimentation\n",
    "# ============================================================================\n",
    "# Training on the full dataset takes hours on a single GPU.\n",
    "# Start with 10% to validate your pipeline, then scale up.\n",
    "\n",
    "SUBSET_FRACTION = 0.1  # Use 10% of the data  (change to 1.0 for full run)\n",
    "\n",
    "num_examples = int(len(dataset) * SUBSET_FRACTION)\n",
    "dataset = dataset.shuffle(seed=42).select(range(num_examples))\n",
    "print(f\"Using {SUBSET_FRACTION:.0%} subset: {len(dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prompt Engineering\n",
    "\n",
    "LLMs learn from **structured prompts**. We convert each raw example into an instruction format that teaches the model:\n",
    "\n",
    "```\n",
    "Below is a question about a database along with the schema.\n",
    "Write the SQL query that answers the question.\n",
    "\n",
    "### Question:\n",
    "{the actual question}\n",
    "\n",
    "### Schema:\n",
    "{CREATE TABLE statements}\n",
    "\n",
    "### SQL:\n",
    "{the gold answer}\n",
    "```\n",
    "\n",
    "**Why this format?** The model learns the pattern: when it sees `### SQL:`, it should generate a SQL query based on the question and schema above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Format Prompts\n",
    "# ============================================================================\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Below is a question about a database along with the schema.\n",
    "Write the SQL query that answers the question.\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Schema:\n",
    "{context}\n",
    "\n",
    "### SQL:\n",
    "{answer}\"\"\"\n",
    "\n",
    "\n",
    "def format_example(example):\n",
    "    \"\"\"\n",
    "    Convert a dataset row into an instruction-style prompt.\n",
    "    \n",
    "    The .format() method replaces {question}, {context}, {answer}\n",
    "    with the actual values from the dataset row.\n",
    "    \"\"\"\n",
    "    example[\"text\"] = PROMPT_TEMPLATE.format(\n",
    "        question=example[\"question\"],\n",
    "        context=example[\"context\"],\n",
    "        answer=example[\"answer\"],\n",
    "    )\n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply to every example using .map()\n",
    "# .map() is the standard way to transform datasets in Hugging Face.\n",
    "# It's lazy, cached, and supports multiprocessing.\n",
    "dataset = dataset.map(format_example)\n",
    "\n",
    "# Let's see what one looks like\n",
    "print(\"=\" * 60)\n",
    "print(\"FORMATTED PROMPT EXAMPLE:\")\n",
    "print(\"=\" * 60)\n",
    "print(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3b: Split into Train / Validation / Test\n",
    "# ============================================================================\n",
    "# The dataset comes as a single split, so we create our own:\n",
    "#   - Train (80%): used for gradient updates\n",
    "#   - Validation (10%): used for early stopping / hyper-param tuning\n",
    "#   - Test (10%): used for final evaluation (never seen during training)\n",
    "\n",
    "# First, carve out the test set\n",
    "train_val_test = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Then split the remainder into train and validation\n",
    "train_val = train_val_test[\"train\"].train_test_split(test_size=0.111, seed=42)\n",
    "# 0.111 of 0.9 â‰ˆ 0.1 of the total â†’ 10% validation\n",
    "\n",
    "train_dataset = train_val[\"train\"]\n",
    "val_dataset = train_val[\"test\"]\n",
    "test_dataset = train_val_test[\"test\"]\n",
    "\n",
    "print(f\"Train:      {len(train_dataset):,} examples\")\n",
    "print(f\"Validation: {len(val_dataset):,} examples\")\n",
    "print(f\"Test:       {len(test_dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load the Model with QLoRA Quantisation\n",
    "\n",
    "### What is QLoRA?\n",
    "\n",
    "**QLoRA = Quantised LoRA** â€” it combines two techniques:\n",
    "\n",
    "1. **Quantisation** (BitsAndBytes): Compress the 7B model from fp16 (14 GB) to 4-bit NF4 (~4.5 GB)\n",
    "2. **LoRA**: Train small adapter matrices (~50 MB) on top of the frozen, quantised base\n",
    "\n",
    "This means you can fine-tune a 7B model on a **single consumer GPU** (16 GB VRAM)!\n",
    "\n",
    "### Key Quantisation Concepts\n",
    "\n",
    "| Concept | What It Does |\n",
    "|---------|-------------|\n",
    "| **NF4** (Normal Float 4) | 4-bit data type optimised for normally-distributed weights |\n",
    "| **Double Quantisation** | Quantises the quantisation constants too (saves ~0.4 GB) |\n",
    "| **Compute dtype** | The precision used during forward/backward (fp16 for speed) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4a: Configure 4-bit Quantisation\n",
    "# ============================================================================\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# BitsAndBytesConfig tells from_pretrained() HOW to load the weights.\n",
    "# Instead of loading 16-bit floats, it loads 4-bit NF4 integers.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     # Enable 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",             # NF4 is best for normally-distributed weights\n",
    "    bnb_4bit_compute_dtype=torch.float16,   # Compute in fp16 during forward pass\n",
    "    bnb_4bit_use_double_quant=True,         # Quantise the quantisation constants too\n",
    ")\n",
    "\n",
    "print(\"Quantisation config created:\")\n",
    "print(f\"  - 4-bit NF4 quantisation\")\n",
    "print(f\"  - Double quantisation: ON\")\n",
    "print(f\"  - Compute dtype: float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4b: Load the Base Model (Quantised)\n",
    "# ============================================================================\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Load the tokeniser\n",
    "# The tokeniser converts text â†’ token IDs and back.\n",
    "# It's separate from the model and is NOT quantised.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# IMPORTANT: Many models don't define a pad token.\n",
    "# Without it, batched training crashes.  We use EOS as a safe fallback.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Pad on the right for causal LM\n",
    "\n",
    "# Load the model with quantisation\n",
    "# device_map=\"auto\" places layers across available GPUs automatically.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Disable caching (incompatible with gradient checkpointing)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Check model size\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel loaded: {MODEL_NAME}\")\n",
    "print(f\"Total parameters: {total_params / 1e9:.2f}B\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Attach LoRA Adapters\n",
    "\n",
    "### How LoRA Works\n",
    "\n",
    "Instead of updating all 7B parameters, LoRA inserts **small trainable matrices** into specific layers:\n",
    "\n",
    "```\n",
    "W' = W + (Î±/r) Ã— B @ A\n",
    "\n",
    "where:\n",
    "  W  âˆˆ R^{dÃ—k}  â€” original frozen weight (e.g. 4096Ã—4096)\n",
    "  A  âˆˆ R^{dÃ—r}  â€” down-projection (trainable)\n",
    "  B  âˆˆ R^{rÃ—k}  â€” up-projection (trainable, initialised to 0)\n",
    "  r              â€” rank (e.g. 16, much smaller than d)\n",
    "  Î±              â€” scaling factor\n",
    "```\n",
    "\n",
    "**Parameter savings:**\n",
    "- Original layer: 4096 Ã— 4096 = 16.8M parameters\n",
    "- LoRA adapter (r=16): 4096Ã—16 + 16Ã—4096 = 131K parameters\n",
    "- **That's 0.78% of the original!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: Configure and Attach LoRA Adapters\n",
    "# ============================================================================\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Step 5a: Prepare the quantised model for training\n",
    "# This casts layer norms to float32 for numerical stability\n",
    "# and enables gradient computation on input embeddings.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Step 5b: Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Rank: dimensionality of A and B matrices\n",
    "    lora_alpha=32,           # Scaling factor (rule of thumb: 2 Ã— r)\n",
    "    lora_dropout=0.05,       # Dropout on LoRA layers (regularisation)\n",
    "    target_modules=[         # Which layers get LoRA adapters\n",
    "        \"q_proj\",            # Attention: query projection\n",
    "        \"k_proj\",            # Attention: key projection\n",
    "        \"v_proj\",            # Attention: value projection\n",
    "        \"o_proj\",            # Attention: output projection\n",
    "        \"gate_proj\",         # MLP: gate projection (SwiGLU)\n",
    "        \"up_proj\",           # MLP: up projection\n",
    "        \"down_proj\",         # MLP: down projection\n",
    "    ],\n",
    "    bias=\"none\",             # Don't train bias terms\n",
    "    task_type=\"CAUSAL_LM\",   # Task type for causal language modelling\n",
    ")\n",
    "\n",
    "# Step 5c: Wrap the model â€” freezes base, makes LoRA trainable\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print the parameter summary\n",
    "model.print_trainable_parameters()\n",
    "# Expected output:\n",
    "# trainable params: 13,631,488 || all params: 3,514,900,480 || trainable%: 0.39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training\n",
    "\n",
    "We use **SFTTrainer** from the `trl` library â€” a Trainer subclass designed for Supervised Fine-Tuning.\n",
    "\n",
    "### Key Training Parameters\n",
    "\n",
    "| Parameter | Value | Why |\n",
    "|-----------|-------|-----|\n",
    "| `learning_rate` | 2e-4 | Standard for QLoRA; higher than full fine-tuning |\n",
    "| `batch_size Ã— grad_accum` | 4 Ã— 4 = 16 | Effective batch of 16 without needing 16Ã— memory |\n",
    "| `num_epochs` | 3 | 1â€“3 is typical for fine-tuning |\n",
    "| `lr_scheduler` | cosine | Smooth decay to near-zero |\n",
    "| `gradient_checkpointing` | ON | Saves ~30% VRAM by recomputing activations |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: Configure Training & Launch\n",
    "# ============================================================================\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Training arguments control EVERYTHING about the training loop:\n",
    "# batch size, learning rate, checkpointing, logging, etc.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                   # Where to save checkpoints\n",
    "    num_train_epochs=3,                        # Total passes over the dataset\n",
    "    per_device_train_batch_size=4,             # Batch size PER GPU\n",
    "    per_device_eval_batch_size=4,              # Eval batch size\n",
    "    gradient_accumulation_steps=4,             # Effective batch = 4 Ã— 4 = 16\n",
    "    learning_rate=2e-4,                        # Peak learning rate\n",
    "    weight_decay=0.001,                        # L2 regularisation\n",
    "    lr_scheduler_type=\"cosine\",                # Cosine annealing schedule\n",
    "    warmup_ratio=0.03,                         # Warm up for 3% of total steps\n",
    "    fp16=True,                                 # Mixed precision (float16)\n",
    "    gradient_checkpointing=True,               # Trade compute for memory\n",
    "    logging_steps=25,                          # Log every 25 steps\n",
    "    save_strategy=\"steps\",                     # Save checkpoints by step count\n",
    "    save_steps=200,                            # Save every 200 steps\n",
    "    save_total_limit=3,                        # Keep only last 3 checkpoints\n",
    "    eval_strategy=\"steps\",                     # Evaluate by step count\n",
    "    eval_steps=200,                            # Evaluate every 200 steps\n",
    "    optim=\"paged_adamw_32bit\",                 # QLoRA-friendly optimiser\n",
    "    seed=42,                                   # Reproducibility\n",
    "    group_by_length=True,                      # Group similar lengths (less padding)\n",
    "    report_to=\"none\",                          # Set to \"wandb\" to enable tracking\n",
    "    remove_unused_columns=False,               # Keep all dataset columns\n",
    ")\n",
    "\n",
    "# SFTTrainer handles:\n",
    "# - Causal LM loss computation (labels = shifted input_ids)\n",
    "# - Sequence packing (multiple short examples in one sequence)\n",
    "# - PEFT integration (only computes gradients for LoRA params)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    max_seq_length=512,              # Max tokens per sequence\n",
    "    dataset_text_field=\"text\",       # Column with our formatted prompts\n",
    ")\n",
    "\n",
    "print(\"Trainer initialised. Ready to train!\")\n",
    "print(f\"  Training examples:   {len(train_dataset):,}\")\n",
    "print(f\"  Validation examples: {len(val_dataset):,}\")\n",
    "print(f\"  Effective batch size: {4 * 4} = batch_size Ã— grad_accumulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6b: Train!\n",
    "# ============================================================================\n",
    "# This is where the magic happens.  The Trainer will:\n",
    "#   1. Iterate over batches of training data\n",
    "#   2. Compute the causal LM loss (cross-entropy)\n",
    "#   3. Backpropagate gradients through the LoRA matrices\n",
    "#   4. Update the LoRA weights with PagedAdamW\n",
    "#   5. Log loss/learning_rate/GPU memory to the console (and W&B)\n",
    "#   6. Save checkpoints every N steps\n",
    "#   7. Run evaluation on the validation set\n",
    "#\n",
    "# Expected time: ~30â€“60 min on T4 (10% subset, 3 epochs)\n",
    "\n",
    "print(\"ğŸ‹ï¸ Starting training ...\")\n",
    "trainer.train()\n",
    "print(\"âœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6c: Save the Adapter and Merged Model\n",
    "# ============================================================================\n",
    "\n",
    "# Save JUST the LoRA adapter weights (~50 MB)\n",
    "# You can reload these later with PeftModel.from_pretrained()\n",
    "adapter_path = \"./results/adapter\"\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"ğŸ’¾ Adapter saved to {adapter_path}\")\n",
    "\n",
    "# Merge the adapter INTO the base model and save as a standalone model\n",
    "# This \"bakes in\" the LoRA weights: W_merged = W_base + (Î±/r) Ã— B @ A\n",
    "# The merged model can be loaded without the PEFT library.\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_path = \"./results/merged_model\"\n",
    "merged_model.save_pretrained(merged_path)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "print(f\"ğŸ’¾ Merged model saved to {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Inference â€” Test Your Model!\n",
    "\n",
    "Now let's use the fine-tuned model to generate SQL queries from natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: Generate SQL with the Fine-Tuned Model\n",
    "# ============================================================================\n",
    "\n",
    "# Inference prompt template â€” same as training but WITHOUT the answer\n",
    "INFERENCE_TEMPLATE = \"\"\"Below is a question about a database along with the schema.\n",
    "Write the SQL query that answers the question.\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Schema:\n",
    "{context}\n",
    "\n",
    "### SQL:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_sql(question: str, schema: str, max_new_tokens: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Generate a SQL query from a natural-language question and schema.\n",
    "    \n",
    "    Args:\n",
    "        question: The natural language question\n",
    "        schema: The CREATE TABLE schema(s)\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        The generated SQL query\n",
    "    \"\"\"\n",
    "    prompt = INFERENCE_TEMPLATE.format(question=question, context=schema)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.1,          # Low temperature for deterministic SQL\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "# â”€â”€ Try it out! â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "test_question = \"How many employees are in each department?\"\n",
    "test_schema = \"\"\"CREATE TABLE employees (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    department TEXT,\n",
    "    salary REAL\n",
    ")\"\"\"\n",
    "\n",
    "generated_sql = generate_sql(test_question, test_schema)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Generated SQL: {generated_sql}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 7b: Test on Examples from the Test Set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREDICTIONS ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(min(10, len(test_dataset))):\n",
    "    example = test_dataset[i]\n",
    "    predicted = generate_sql(example[\"question\"], example[\"context\"])\n",
    "    \n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Gold SQL: {example['answer']}\")\n",
    "    print(f\"Pred SQL: {predicted}\")\n",
    "    print(f\"Match:    {'âœ…' if predicted.strip().lower().rstrip(';') == example['answer'].strip().lower().rstrip(';') else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluation\n",
    "\n",
    "We evaluate using multiple complementary metrics:\n",
    "\n",
    "| Metric | What It Measures |\n",
    "|--------|------------------|\n",
    "| **Perplexity** | How surprised the model is by the test data (lower = better) |\n",
    "| **Exact Match** | Does the prediction exactly match the gold SQL? |\n",
    "| **Execution Accuracy** | Is the generated SQL syntactically valid? |\n",
    "| **ROUGE-L** | Longest common subsequence overlap (partial credit) |\n",
    "| **BLEU** | N-gram precision (standard text generation metric) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: Evaluate on the Test Set\n",
    "# ============================================================================\n",
    "import sqlparse\n",
    "import math\n",
    "\n",
    "# Limit evaluation examples for speed\n",
    "MAX_EVAL = 100\n",
    "eval_subset = test_dataset.select(range(min(MAX_EVAL, len(test_dataset))))\n",
    "\n",
    "# Generate predictions for all test examples\n",
    "print(f\"Generating predictions for {len(eval_subset)} test examples ...\")\n",
    "predictions = []\n",
    "for i, example in enumerate(eval_subset):\n",
    "    pred = generate_sql(example[\"question\"], example[\"context\"])\n",
    "    predictions.append(pred)\n",
    "    if (i + 1) % 25 == 0:\n",
    "        print(f\"  [{i+1}/{len(eval_subset)}] done\")\n",
    "\n",
    "gold_answers = eval_subset[\"answer\"]\n",
    "print(f\"âœ… Generated {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 8b: Compute All Metrics\n",
    "# ============================================================================\n",
    "import evaluate\n",
    "\n",
    "def normalise_sql(sql):\n",
    "    \"\"\"Normalise SQL for fair comparison.\"\"\"\n",
    "    return sql.strip().lower().rstrip(\";\").strip()\n",
    "\n",
    "# â”€â”€ Exact Match â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "exact_matches = sum(\n",
    "    1 for p, g in zip(predictions, gold_answers)\n",
    "    if normalise_sql(p) == normalise_sql(g)\n",
    ")\n",
    "exact_match_acc = exact_matches / len(predictions)\n",
    "\n",
    "# â”€â”€ Execution Accuracy (Syntax Check) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "valid_sql = 0\n",
    "for sql in predictions:\n",
    "    try:\n",
    "        parsed = sqlparse.parse(sql.strip())\n",
    "        if parsed and str(parsed[0]).strip():\n",
    "            valid_sql += 1\n",
    "    except Exception:\n",
    "        pass\n",
    "exec_acc = valid_sql / len(predictions)\n",
    "\n",
    "# â”€â”€ ROUGE-L â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "rouge_results = rouge.compute(\n",
    "    predictions=predictions, references=gold_answers, rouge_types=[\"rougeL\"]\n",
    ")\n",
    "\n",
    "# â”€â”€ BLEU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_results = bleu.compute(\n",
    "    predictions=predictions, references=[[g] for g in gold_answers]\n",
    ")\n",
    "\n",
    "# â”€â”€ Print Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“Š EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Exact Match:         {exact_match_acc:.2%} ({exact_matches}/{len(predictions)})\")\n",
    "print(f\"  Execution Accuracy:  {exec_acc:.2%} ({valid_sql}/{len(predictions)})\")\n",
    "print(f\"  ROUGE-L:             {rouge_results['rougeL']:.4f}\")\n",
    "print(f\"  BLEU:                {bleu_results['bleu']:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "You've successfully fine-tuned a 7B parameter LLM using QLoRA!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Scale up**: Set `SUBSET_FRACTION = 1.0` and train on the full dataset\n",
    "2. **Experiment**: Try different LoRA ranks (8, 32, 64) and see how it affects quality\n",
    "3. **Deploy**: Use the `inference.py` script to serve your model via vLLM or FastAPI\n",
    "4. **Track**: Enable Weights & Biases (`report_to=\"wandb\"`) to compare experiments\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| What | How |\n",
    "|------|-----|\n",
    "| Memory savings | 4-bit quantisation: 14 GB â†’ 4.5 GB |\n",
    "| Parameter efficiency | LoRA: train 0.4% of parameters |\n",
    "| Training speed | 3 epochs on 10% data in ~30 min (T4) |\n",
    "| Serving | vLLM for 2â€“4Ã— throughput vs HF generate() |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
