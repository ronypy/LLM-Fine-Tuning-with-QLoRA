# ğŸ§  LLM Fine-Tuning Pipeline â€” Text-to-SQL with QLoRA

Fine-tune **Mistral-7B** to generate SQL queries from natural language using **QLoRA** (4-bit quantised LoRA), evaluate with standard benchmarks, and serve via **vLLM** or **FastAPI**.

![Python](https://img.shields.io/badge/Python-3.9+-blue)
![HuggingFace](https://img.shields.io/badge/ğŸ¤—-Transformers-yellow)
![License](https://img.shields.io/badge/License-MIT-green)

---

## ğŸ“‹ Table of Contents

- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [How QLoRA Works](#-how-qlora-works)
- [Training](#-training)
- [Inference & Serving](#-inference--serving)
- [Evaluation](#-evaluation)
- [Results](#-results)
- [Tech Stack](#-tech-stack)
- [Configuration Reference](#-configuration-reference)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRAINING PIPELINE                              â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ HF Datasets  â”‚ â†’ â”‚ data_prep.py â”‚ â†’ â”‚ Formatted Prompts    â”‚  â”‚
â”‚  â”‚ sql-create-  â”‚   â”‚  â€¢ Load      â”‚   â”‚ "### Question: ..."  â”‚  â”‚
â”‚  â”‚ context      â”‚   â”‚  â€¢ Format    â”‚   â”‚ "### Schema: ..."    â”‚  â”‚
â”‚  â”‚              â”‚   â”‚  â€¢ Split     â”‚   â”‚ "### SQL: ..."       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                   â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚              â”‚
â”‚  â”‚ Mistral-7B  â”‚ â†’ â”‚  train.py    â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚  â”‚ (4-bit NF4) â”‚   â”‚  â€¢ QLoRA     â”‚                             â”‚
â”‚  â”‚  ~4.5 GB    â”‚   â”‚  â€¢ SFTTrainerâ”‚                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â€¢ W&B logs  â”‚                             â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                            â”‚                                     â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                     â”‚ Merged Model â”‚                             â”‚
â”‚                     â”‚ + Adapter    â”‚                             â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INFERENCE PIPELINE                             â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ User Query   â”‚ â†’ â”‚ inference.py â”‚ â†’ â”‚ Generated SQL        â”‚ â”‚
â”‚  â”‚ "How many    â”‚   â”‚              â”‚   â”‚ SELECT COUNT(*)      â”‚ â”‚
â”‚  â”‚  employees?" â”‚   â”‚ Modes:       â”‚   â”‚ FROM employees       â”‚ â”‚
â”‚  â”‚              â”‚   â”‚ â€¢ Local REPL â”‚   â”‚ WHERE dept = 5       â”‚ â”‚
â”‚  â”‚              â”‚   â”‚ â€¢ FastAPI    â”‚   â”‚                      â”‚ â”‚
â”‚  â”‚              â”‚   â”‚ â€¢ vLLM       â”‚   â”‚                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### 1. Clone & Install

```bash
git clone <your-repo-url>
cd llm-finetuning

# Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Prepare Data

```bash
# Preview formatted prompts (no GPU needed)
python src/data_prep.py --config configs/lora_config.yaml --peek

# Full processing with train/val/test split
python src/data_prep.py --config configs/lora_config.yaml --output data/processed
```

### 3. Train

```bash
# Train with pre-processed data
python src/train.py --config configs/lora_config.yaml --data data/processed

# Or train with on-the-fly processing
python src/train.py --config configs/lora_config.yaml
```

### 4. Run Inference

```bash
# Interactive REPL
python src/inference.py --mode local --model results/merged_model

# FastAPI server (with Swagger docs at /docs)
python src/inference.py --mode api --model results/merged_model --port 8000

# High-throughput vLLM server
python src/inference.py --mode vllm --model results/merged_model --port 8000
```

### 5. Evaluate

```bash
python evaluation/benchmark.py \
    --model results/merged_model \
    --test-data data/processed \
    --config configs/lora_config.yaml \
    --output evaluation/results.json
```

---

## ğŸ“ Project Structure

```
llm-finetuning/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ lora_config.yaml      # All hyper-parameters in one place
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_prep.py           # Dataset loading, formatting, tokenisation
â”‚   â”œâ”€â”€ train.py               # QLoRA training with SFTTrainer
â”‚   â””â”€â”€ inference.py           # Local REPL, FastAPI, vLLM serving
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ benchmark.py           # Perplexity, Exact Match, ROUGE, BLEU
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ training.ipynb         # Interactive walkthrough (Colab-ready)
â”œâ”€â”€ requirements.txt           # Pinned dependencies
â””â”€â”€ README.md                  # This file
```

---

## ğŸ”¬ How QLoRA Works

### The Memory Problem

A 7B-parameter model in fp16 requires **14 GB** of VRAM just for weights â€” before counting gradients, optimiser states, and activations.

### The Solution: QLoRA

QLoRA combines **two** techniques:

#### 1. 4-bit Quantisation (BitsAndBytes)

Compress each weight from 16 bits â†’ 4 bits using **NF4** (Normal Float 4), a data type optimised for normally-distributed neural network weights.

```
fp16:  7B Ã— 2 bytes  = 14.0 GB
NF4:   7B Ã— 0.5 bytes =  3.5 GB  (+overhead â‰ˆ 4.5 GB total)
```

#### 2. Low-Rank Adaptation (LoRA)

Instead of updating all 7B parameters, insert small trainable matrices:

```
W' = W + (Î±/r) Ã— B @ A

W âˆˆ R^{4096Ã—4096}  â€” frozen base weight  (16.8M params)
A âˆˆ R^{4096Ã—16}    â€” down-projection     (65K params)   â† trainable
B âˆˆ R^{16Ã—4096}    â€” up-projection       (65K params)   â† trainable
```

**Result**: Train **0.4% of parameters** with **~4.5 GB VRAM**.

---

## ğŸ‹ï¸ Training

### Key Configuration (from `lora_config.yaml`)

| Parameter | Default | Description |
|-----------|---------|-------------|
| `model.name` | `mistralai/Mistral-7B-v0.1` | Base model |
| `model.quantization_bits` | 4 | QLoRA precision |
| `lora.r` | 16 | LoRA rank |
| `lora.lora_alpha` | 32 | Scaling factor (2Ã—r) |
| `training.learning_rate` | 2e-4 | Peak LR |
| `training.num_train_epochs` | 3 | Total epochs |
| `training.per_device_train_batch_size` | 4 | Batch per GPU |
| `training.gradient_accumulation_steps` | 4 | Effective batch = 16 |
| `data.subset_fraction` | 0.1 | Use 10% for quick runs |

### Experiment Tracking

Enable Weights & Biases by setting `wandb.enabled: true` in the config. Metrics logged:

- Training loss (per step)
- Validation loss (per eval step)
- Learning rate schedule
- GPU memory usage

---

## ğŸŒ Inference & Serving

### Three Serving Options

| Mode | Command | Best For |
|------|---------|----------|
| **Local REPL** | `--mode local` | Development, quick testing |
| **FastAPI** | `--mode api` | Custom endpoints, Swagger docs |
| **vLLM** | `--mode vllm` | Production, high throughput |

### vLLM Advantages

- **2â€“4Ã— higher throughput** via PagedAttention
- **Continuous batching**: new requests don't wait
- **OpenAI-compatible API**: drop-in replacement

### API Example

```bash
# With FastAPI running:
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "question": "How many employees in department 5?",
    "schema_text": "CREATE TABLE employees (id INT, name TEXT, dept_id INT)"
  }'

# Response:
# {"sql": "SELECT COUNT(*) FROM employees WHERE dept_id = 5", "latency_ms": 42.5}
```

---

## ğŸ“Š Evaluation

### Metrics

| Metric | What It Measures | Range |
|--------|-----------------|-------|
| **Perplexity** | Language modelling quality (lower = better) | 1.0 â€“ âˆ |
| **Exact Match** | Predicted SQL â‰¡ gold SQL | 0% â€“ 100% |
| **Execution Accuracy** | Is the SQL syntactically valid? | 0% â€“ 100% |
| **ROUGE-L** | Longest common subsequence overlap | 0.0 â€“ 1.0 |
| **BLEU** | N-gram precision | 0.0 â€“ 1.0 |

---

## ğŸ“ˆ Results

### Before vs After Fine-Tuning

| Metric | Base (Mistral-7B) | Fine-Tuned (QLoRA) |
|--------|-------------------|---------------------|
| Perplexity | ~15â€“20 | ~3â€“5 |
| Exact Match | ~5â€“10% | ~40â€“60% |
| Execution Accuracy | ~60% | ~90%+ |
| ROUGE-L | ~0.30 | ~0.70+ |
| BLEU | ~0.15 | ~0.55+ |

> **Note**: Actual results depend on dataset size, epochs, and hyperparameters.
> Run `evaluation/benchmark.py` to get your own numbers.

### Training Cost

| GPU | 10% Data (3 epochs) | Full Data (3 epochs) |
|-----|---------------------|----------------------|
| T4 (16 GB) | ~30 min | ~5 hours |
| A10 (24 GB) | ~15 min | ~2.5 hours |
| A100 (80 GB) | ~5 min | ~45 min |

---

## ğŸ› ï¸ Tech Stack

| Tool | Purpose |
|------|---------|
| [Transformers](https://huggingface.co/docs/transformers) | Model loading, tokenisation, Trainer API |
| [PEFT](https://huggingface.co/docs/peft) | LoRA / QLoRA adapter management |
| [TRL](https://huggingface.co/docs/trl) | SFTTrainer for supervised fine-tuning |
| [Datasets](https://huggingface.co/docs/datasets) | Efficient data loading from HF Hub |
| [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes) | 4-bit / 8-bit quantisation |
| [vLLM](https://github.com/vllm-project/vllm) | High-throughput inference with PagedAttention |
| [FastAPI](https://fastapi.tiangolo.com/) | REST API with auto-generated docs |
| [Weights & Biases](https://wandb.ai/) | Experiment tracking and visualisation |
| [sqlparse](https://github.com/andialbrecht/sqlparse) | SQL syntax validation for evaluation |

---

## âš™ï¸ Configuration Reference

All parameters are in [`configs/lora_config.yaml`](configs/lora_config.yaml). Key sections:

```yaml
model:
  name: "mistralai/Mistral-7B-v0.1"
  quantization_bits: 4            # 4 = QLoRA, 8 = LoRA-8bit, 16 = full

lora:
  r: 16                           # Rank (try 8, 16, 32, 64)
  lora_alpha: 32                  # Scaling (rule of thumb: 2 Ã— r)
  target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]

training:
  num_train_epochs: 3
  learning_rate: 2.0e-4
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4  # Effective batch = 16

data:
  dataset_name: "b-mc2/sql-create-context"
  subset_fraction: 0.1            # Start small, scale up
```

---

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.
